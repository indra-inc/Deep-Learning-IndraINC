{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43928c2e",
   "metadata": {},
   "source": [
    "## RNN:\n",
    "- It is a generalisation of feedforward neural network that has an internal memory\n",
    "- It is recurrent in nature as it performs same function for every input data while output of the current input depends on the past one computing\n",
    "- For making the decision , it considers the current input and the output that it has learned from the previous input\n",
    "- It can use their internal state (memory) to process sequences of inputs\n",
    "- In other ANN all the inputs are independent of each other but RNN all the inputs are related to each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36731b95",
   "metadata": {},
   "source": [
    "## Basic LSTM Model\n",
    "- LSTM networks are a modified version of RNN which makes it easier to remember past data in memory\n",
    "- The Vanishing gradient problem of RNN is resolved\n",
    "- LSTM is well suited to classify, process and predict time series given time lags of unknown duration\n",
    "- It trains the model using Backpropagation\n",
    "- Here to understand LSTM layer we are using IMDB dataset to predict whether a movie review from IMDB is generally positive(1) and negative(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024280c",
   "metadata": {},
   "source": [
    "## IMDB (Internet Movie DataBase) dataset to predict Movie Reviews \n",
    "- The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive(1) or negative(0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542daf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb13182",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary libraries \n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dcdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using random seed for reproducibilty\n",
    "np.random.seed(500)\n",
    "\n",
    "# creating training and test dataset\n",
    "(x_train,y_train), (x_test,y_test) = imdb.load_data(num_words = 50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb49ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2177a72",
   "metadata": {},
   "source": [
    "### Commnt : By default, the dataset is splitted in 50-50 ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039dc32",
   "metadata": {},
   "source": [
    "### Padding and truncation of sequences\n",
    "\n",
    "- When developing the author classification model, the number of integers for each training and test text data need to be of equal length. We can achieve this by padding and truncating the sequence of integers, as follows:\n",
    "\n",
    "- Padding and truncationtrainx <- pad_sequences(trainx, maxlen = 300) testx <- pad_sequences(testx, maxlen = 300)dim(trainx) [1] 2500  300\n",
    "\n",
    "- Here, we are specifying the maximum length of all the sequences, that is, maxlen, to be 300. This will truncate any sequences that are longer than 300 integers in an article and add zeroes to sequences that are shorter than 300 integers in an article. Note that for padding and truncation, a default setting of \"pre\" has been used and is not specifically ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6496dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen = 200)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9d1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(1000, 32, input_length = 200))  ## Embedded layer that uses 32 length vectors to represent each word\n",
    "model_1.add(LSTM(100))  ## LSTM layer with 100 memory unit\n",
    "model_1.add(Dense(1, activation = 'relu'))    ## Relu activation as because classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bdf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 32)           32000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,301\n",
      "Trainable params: 85,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Compile the model\n",
    "model_1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d6bdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/embedding/embedding_lookup' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\INDRAI~1\\AppData\\Local\\Temp/ipykernel_336/1244510772.py\", line 2, in <module>\n      model_1.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 3, batch_size = 1000)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential/embedding/embedding_lookup'\nindices[875,47] = 1010 is not in [0, 1000)\n\t [[{{node sequential/embedding/embedding_lookup}}]] [Op:__inference_train_function_2717]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\INDRAI~1\\AppData\\Local\\Temp/ipykernel_336/1244510772.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/embedding/embedding_lookup' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\INDRAI~1\\AppData\\Local\\Temp/ipykernel_336/1244510772.py\", line 2, in <module>\n      model_1.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 3, batch_size = 1000)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential/embedding/embedding_lookup'\nindices[875,47] = 1010 is not in [0, 1000)\n\t [[{{node sequential/embedding/embedding_lookup}}]] [Op:__inference_train_function_2717]"
     ]
    }
   ],
   "source": [
    "## Fit the model\n",
    "model_1.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 3, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5446eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the model\n",
    "scores = model_1.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Accuracy_Model_1: %.2f%%'% (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8111a6f",
   "metadata": {},
   "source": [
    "### Changing Activation Function : Relu to Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(1000, 32, input_length = 200))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "## Compile the model\n",
    "model_2.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model_2.summary())\n",
    "print('')\n",
    "## Fit the model\n",
    "model_2.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 3, batch_size = 1000)\n",
    "\n",
    "## Evaluating the model\n",
    "scores = model_2.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Accuracy_Model_2: %.2f%%'% (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5263c56",
   "metadata": {},
   "source": [
    "### Adding Dropout : Model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Model\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(1000, 32, input_length = 200))\n",
    "model_3.add(dropout(0.2))\n",
    "model_3.add(LSTM(100))\n",
    "model_3.add(dropout(0.2))\n",
    "model_3.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "## Compile the model\n",
    "model_3.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model_3.summary())\n",
    "print('')\n",
    "## Fit the model\n",
    "model_3.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 3, batch_size = 1000)\n",
    "\n",
    "## Evaluating the model\n",
    "scores = model_3.evaluate(x_test, y_test)\n",
    "print('Test Accuracy_Model_3: %.2f%%'% (scores[1]*100))\n",
    "print('')\n",
    "scores_tr = model_3.evaluate(x_train, y_train)\n",
    "print('Train Accuracy_Model_3: %.2f%%'% (scores_tr[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f22da",
   "metadata": {},
   "source": [
    "### Adding Recurrent Dropout and adding more layer : Model_4\n",
    "- Applying dropout to the input and recurrent connections of the memory units\n",
    "- Recurrent dropout is a specific, built-in way to use dropout to fight overfitting in recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a76da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Model\n",
    "model_4 = Sequential()\n",
    "model_4.add(Embedding(1000, 32, input_length = 200))\n",
    "model_4.add(LSTM(100, dropout = 0.2, recurrent_dropout = 0.1))\n",
    "model_4.add(Dense (500, activation = 'sigmoid'))\n",
    "model_4.add(Dense (100, activation = 'sigmoid'))\n",
    "model_4.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "## Compile the model\n",
    "model_4.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model_4.summary())\n",
    "print('')\n",
    "## Fit the model\n",
    "model_4.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 3, batch_size = 1000)\n",
    "\n",
    "## Evaluating the model\n",
    "scores = model_4.evaluate(x_test, y_test)\n",
    "print('Test Accuracy_Model_4: %.2f%%'% (scores[1]*100))\n",
    "print('')\n",
    "scores_tr = model_4.evaluate(x_train, y_train)\n",
    "print('Train Accuracy_Model_4: %.2f%%'% (scores_tr[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a52684",
   "metadata": {},
   "source": [
    "### Adding Conv1D Layer for Sequence Classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8fb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Model\n",
    "model_5 = Sequential()\n",
    "model_5.add(Embedding(1000, 32, input_length = 200))\n",
    "model_5.add(Conv1D(filters= 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model_5.add(MaxPooling1D(pool_size = 2))\n",
    "model_5.add(LSTM(100, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model_5.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "## Compile the model\n",
    "model_5.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model_5.summary())\n",
    "print('')\n",
    "## Fit the model\n",
    "model_5.fit(x_train, y_train, epochs = 3, batch_size = 64)\n",
    "\n",
    "## Evaluating the model\n",
    "scores = model_5.evaluate(x_test, y_test)\n",
    "print('Test Accuracy_Model_5: %.2f%%'% (scores[1]*100))\n",
    "print('')\n",
    "scores_tr = model_5.evaluate(x_train, y_train)\n",
    "print('Train Accuracy_Model_5: %.2f%%'% (scores_tr[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed659c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005343b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e9581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb858c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f9079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b463cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
