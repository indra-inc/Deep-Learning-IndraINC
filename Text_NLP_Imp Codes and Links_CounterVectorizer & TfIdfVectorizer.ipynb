{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd06e5e3",
   "metadata": {},
   "source": [
    "## CountVectorizer :\n",
    "- CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. This is helpful when we have multiple such texts, and we wish to convert each word in each text into vectors (for using in further text analysis).\n",
    "- CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample. \n",
    "- Inside CountVectorizer, these words are not stored as strings. Rather, they are given a particular index value. \n",
    "- All words have been converted to lowercase.\n",
    "- The words in columns have been arranged alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aa7cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   big  count  create  dataset  differnt  features  hello  is  james  my  \\\n",
      "0    0      0       0        0         0         0      1   1      1   1   \n",
      "1    0      0       0        0         0         0      0   1      1   1   \n",
      "2    1      0       1        1         0         0      0   0      1   0   \n",
      "3    0      0       0        0         1         0      0   0      1   0   \n",
      "4    0      1       0        0         0         1      0   0      0   0   \n",
      "\n",
      "   name  notebook  of  python  this  to  try  trying  vectorizer  words  \n",
      "0     1         0   0       0     0   0    0       0           0      0  \n",
      "1     0         1   0       1     1   0    0       0           0      0  \n",
      "2     0         0   0       0     0   1    0       1           0      0  \n",
      "3     0         0   1       0     0   1    1       0           0      1  \n",
      "4     0         0   1       0     0   0    0       0           1      0  \n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c\n",
    "## Count Vectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = ['Hello my name is james',\n",
    "        'james this is my python notebook',\n",
    "        'james trying to create a big dataset',\n",
    "        'james of words to try differnt',\n",
    "        'features of count vectorizer']\n",
    "coun_vect = CountVectorizer()\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ee0f5",
   "metadata": {},
   "source": [
    "#### Obs : This way of representation is known as a Sparse Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92daf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coun_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cd38b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 27 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "979adb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755388a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'count',\n",
       " 'create',\n",
       " 'dataset',\n",
       " 'differnt',\n",
       " 'features',\n",
       " 'hello',\n",
       " 'is',\n",
       " 'james',\n",
       " 'my',\n",
       " 'name',\n",
       " 'notebook',\n",
       " 'of',\n",
       " 'python',\n",
       " 'this',\n",
       " 'to',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'vectorizer',\n",
       " 'words']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coun_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e446f",
   "metadata": {},
   "source": [
    "## Parameters inside CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f25c2",
   "metadata": {},
   "source": [
    "### Param_1. Lowercase = False or True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7f5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hello  James  hello  is  james  my  name\n",
      "0      0      0      1   1      1   1     1\n",
      "1      1      1      0   1      0   1     1\n"
     ]
    }
   ],
   "source": [
    "# By default, Countvectorizer converts the text to lowercase and uses word-level tokenization.\n",
    "# Default is set to true and takes boolean value\n",
    "# If we don't need as True then pass Lowercase = False\n",
    "text = ['hello my name is james','Hello my name is James']\n",
    "coun_vect = CountVectorizer(lowercase = False)\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34fcced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hello  is  james  my  name\n",
      "0      1   1      1   1     1\n",
      "1      1   1      1   1     1\n",
      "Check the Result when Lowercase = True or bydefault nothing will pass then and Lowercase = False\n"
     ]
    }
   ],
   "source": [
    "text = ['hello my name is james','Hello my name is James']\n",
    "coun_vect = CountVectorizer(lowercase = True)\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)\n",
    "print('Check the Result when Lowercase = True or bydefault nothing will pass then and Lowercase = False')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ac314",
   "metadata": {},
   "source": [
    "### Param_2. Stop_words : There are 3 ways of dealing with stopwords\n",
    "- i) Custom stop words list\n",
    "- ii) sklearn built in stop words list\n",
    "- iii) Using max_df and min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adde5d7",
   "metadata": {},
   "source": [
    "#### i) Custom stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48213cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   big  count  create  dataset  differnt  features  hello  james  name  \\\n",
      "0    0      0       0        0         0         0      1      1     1   \n",
      "1    0      0       0        0         0         0      0      1     0   \n",
      "2    1      0       1        1         0         0      0      1     0   \n",
      "3    0      0       0        0         1         0      0      1     0   \n",
      "4    0      1       0        0         0         1      0      0     0   \n",
      "\n",
      "   notebook  of  python  this  try  trying  vectorizer  words  \n",
      "0         0   0       0     0    0       0           0      0  \n",
      "1         1   0       1     1    0       0           0      0  \n",
      "2         0   0       0     0    0       1           0      0  \n",
      "3         0   1       0     0    1       0           0      1  \n",
      "4         0   1       0     0    0       0           1      0  \n",
      "\n",
      "***** Sparse matrix after removing the words is , to and my *****\n"
     ]
    }
   ],
   "source": [
    "# i) Custom stop words list\n",
    "text = ['Hello my name is james',\n",
    "        'james this is my python notebook',\n",
    "        'james trying to create a big dataset',\n",
    "        'james of words to try differnt',\n",
    "        'features of count vectorizer']\n",
    "coun_vect = CountVectorizer(stop_words = ['is','to','my'])\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)\n",
    "print('')\n",
    "print('***** Sparse matrix after removing the words is , to and my *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eff449",
   "metadata": {},
   "source": [
    "#### ii) sklearn built in stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c85c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   big  count  create  dataset  differnt  features  hello  james  notebook  \\\n",
      "0    0      0       0        0         0         0      1      1         0   \n",
      "1    0      0       0        0         0         0      0      1         1   \n",
      "2    1      0       1        1         0         0      0      1         0   \n",
      "3    0      0       0        0         1         0      0      1         0   \n",
      "4    0      1       0        0         0         1      0      0         0   \n",
      "\n",
      "   python  try  trying  vectorizer  words  \n",
      "0       0    0       0           0      0  \n",
      "1       1    0       0           0      0  \n",
      "2       0    0       1           0      0  \n",
      "3       0    1       0           0      1  \n",
      "4       0    0       0           1      0  \n",
      "\n",
      "***** Sparse matrix after passing the parameter value = English *****\n"
     ]
    }
   ],
   "source": [
    "# ii) sklearn built in stop words list\n",
    "coun_vect = CountVectorizer(stop_words = 'english')\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)\n",
    "print('')\n",
    "print('***** Sparse matrix after passing the parameter value = English *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d28d9",
   "metadata": {},
   "source": [
    "#### iii) Using max_df (Not more than that): maximum document frequency\n",
    "- Max_df stands for maximum document frequency. Similar to min_df, we can ignore words which occur frequently. These words could be like the word ‘the’ that occur in every document and does not provide and valuable information to our text classification or any other machine learning model and can be safely ignored. Max_df looks at how many documents contain the word and if it exceeds the max_df threshold then it is eliminated from the sparse matrix. This parameter can again 2 types of values, percentage and absolute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76adda09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   big  count  create  dataset  differnt  features  hello  name  notebook  \\\n",
      "0    0      0       0        0         0         0      1     1         0   \n",
      "1    0      0       0        0         0         0      0     0         1   \n",
      "2    1      0       1        1         0         0      0     0         0   \n",
      "3    0      0       0        0         1         0      0     0         0   \n",
      "4    0      1       0        0         0         1      0     0         0   \n",
      "\n",
      "   python  this  try  trying  vectorizer  words  \n",
      "0       0     0    0       0           0      0  \n",
      "1       1     1    0       0           0      0  \n",
      "2       0     0    0       1           0      0  \n",
      "3       0     0    1       0           0      1  \n",
      "4       0     0    0       0           1      0  \n"
     ]
    }
   ],
   "source": [
    "# iii) Using max_df : Using absolute values (it check column wise and count)\n",
    "coun_vect = CountVectorizer(max_df = 1)\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f89ca9",
   "metadata": {},
   "source": [
    "#### Obs : The words ‘is’, ‘to’, ‘james’, ‘my’ and ‘of’ have been removed from the sparse matrix as they occur in more than 1 document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5894a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   big  count  create  dataset  differnt  features  hello  is  my  name  \\\n",
      "0    0      0       0        0         0         0      1   1   1     1   \n",
      "1    0      0       0        0         0         0      0   1   1     0   \n",
      "2    1      0       1        1         0         0      0   0   0     0   \n",
      "3    0      0       0        0         1         0      0   0   0     0   \n",
      "4    0      1       0        0         0         1      0   0   0     0   \n",
      "\n",
      "   notebook  of  python  this  to  try  trying  vectorizer  words  \n",
      "0         0   0       0     0   0    0       0           0      0  \n",
      "1         1   0       1     1   0    0       0           0      0  \n",
      "2         0   0       0     0   1    0       1           0      0  \n",
      "3         0   1       0     0   1    1       0           0      1  \n",
      "4         0   1       0     0   0    0       0           1      0  \n"
     ]
    }
   ],
   "source": [
    "# iii) Using max_df : Using percentage ( it also check column wise then find the %ge of occurance)\n",
    "coun_vect = CountVectorizer(max_df = 0.75)\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88665374",
   "metadata": {},
   "source": [
    "#### Obs : As you can see the word ‘james’ appears in 4 out of 5 documents(85%) and hence crosses the threshold of 75% and removed from the sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a086ccc",
   "metadata": {},
   "source": [
    "#### iii) Using min_df (Note less than that): minimum document frequency, as opposed to term frequency (TF)\n",
    "- Min_df: Min_df stands for minimum document frequency, as opposed to term frequency which counts the number of times the word has occurred in the entire dataset, document frequency counts the number of documents in the dataset (aka rows or entries) that have the particular word. When building the vocabulary Min_df ignores terms that have a document frequency strictly lower than the given threshold. For example in your dataset you may have names that appear in only 1 or 2 documents, now these could be ignored as they do not provide enough information on the entire dataset as a whole but only a couple of particular documents. min_df can take absolute values(1,2,3..) or a value representing a percentage of documents(0.50, ignore words appearing in 50% of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18803bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is  james  my  of  to\n",
      "0   1      1   1   0   0\n",
      "1   1      1   1   0   0\n",
      "2   0      1   0   0   1\n",
      "3   0      1   0   1   1\n",
      "4   0      0   0   1   0\n"
     ]
    }
   ],
   "source": [
    "# iii) Using min_df : Using absolute values \n",
    "coun_vect = CountVectorizer(min_df = 2)\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a346bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   james\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n"
     ]
    }
   ],
   "source": [
    "# iii) Using max_df : Using percentage \n",
    "coun_vect = CountVectorizer(min_df = 0.5)\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ac421",
   "metadata": {},
   "source": [
    "### Param_3. max_features\n",
    "- The CountVectorizer will select the words/features/terms which occur the most frequently. It takes absolute values so if you set the ‘max_features = 3’, it will select the 3 most common words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "417b9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document  is  the\n",
      "0         1   1    1\n",
      "1         2   1    1\n",
      "2         0   1    1\n",
      "3         1   1    1\n"
     ]
    }
   ],
   "source": [
    "text_1 = ['This is the first document.',\n",
    "        'This document is the second document.',\n",
    "        'And this is the third one.', \n",
    "        'Is this the first document?']\n",
    "coun_vect = CountVectorizer(max_features = 3)\n",
    "count_matrix = coun_vect.fit_transform(text_1)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d4db7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "text_mxf = ['This is the first document.',\n",
    "        'This document is the second document.',\n",
    "        'And this is the third one.', \n",
    "        'Is this the first document?']\n",
    "coun_vect_mxf = CountVectorizer()\n",
    "count_matrix_mxf = coun_vect_mxf.fit_transform(text_mxf)\n",
    "count_array_mxf = count_matrix_mxf.toarray()\n",
    "df_mxf = pd.DataFrame(data = count_array_mxf,columns = coun_vect_mxf.get_feature_names())\n",
    "print(df_mxf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890fb62",
   "metadata": {},
   "source": [
    "### Param_4. Binary\n",
    "- By setting ‘binary = True’, the CountVectorizer no more takes into consideration the frequency of the term/word. If it occurs it’s set to 1 otherwise 0. By default, binary is set to False. This is usually used when the count of the term/word does not provide useful information to the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb170a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document  first  is  the  this\n",
      "0         1      1   1    1     1\n"
     ]
    }
   ],
   "source": [
    "text_2 = ['This is the first document. Is this the first document?']\n",
    "coun_vect = CountVectorizer(binary = True)\n",
    "count_matrix = coun_vect.fit_transform(text_2)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd0385a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document  first  is  the  this\n",
      "0         2      2   2    2     2\n"
     ]
    }
   ],
   "source": [
    "text_3 = ['This is the first document. Is this the first document?']\n",
    "coun_vect = CountVectorizer(binary = False)\n",
    "count_matrix = coun_vect.fit_transform(text_3)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927f9fa",
   "metadata": {},
   "source": [
    "### Param_5. Vocabulary\n",
    "- They are the collection of words in the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5e8eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hello  is  james  my  name\n",
      "0      1   1      1   1     1\n",
      "1      1   1      1   1     1\n",
      "\n",
      "{'hello': 0, 'my': 3, 'name': 4, 'is': 1, 'james': 2}\n"
     ]
    }
   ],
   "source": [
    "text = ['hello my name is james',\n",
    "        'Hello my name is James']\n",
    "coun_vect = CountVectorizer()\n",
    "count_matrix = coun_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data = count_array,columns = coun_vect.get_feature_names())\n",
    "print(df)\n",
    "print('')\n",
    "print(coun_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72218f",
   "metadata": {},
   "source": [
    "#### Obs_imp : The numbers do not represent the count of the words but the position of the words in the matrix\n",
    "- hello is at 0th position\n",
    "- is @ 1st position\n",
    "- james @2nd position\n",
    "- my @3rd position\n",
    "- name @4th position "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52ae9f",
   "metadata": {},
   "source": [
    "#### Comment : CountVectorizer is just one of the methods to deal with textual data. Td-idf is a better method to vectorize data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98734d66",
   "metadata": {},
   "source": [
    "## sklearn’s Tfidfvectorizer Calculates tf-idf\n",
    "- https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "709b2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc19169",
   "metadata": {},
   "source": [
    "#### With Param : stop_words = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dad6603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petrol cars are cheaper than diesel cars', 'diesel is cheaper than petrol']\n",
      "\n",
      "Feature Names n ['cars', 'cheaper', 'diesel', 'petrol']\n",
      "\n",
      "Sparse Matrix n (2, 4) n [[0.85135433 0.30287281 0.30287281 0.30287281]\n",
      " [0.         0.57735027 0.57735027 0.57735027]]\n",
      "\n",
      "       cars   cheaper    diesel    petrol\n",
      "0  0.851354  0.302873  0.302873  0.302873\n",
      "1  0.000000  0.577350  0.577350  0.577350\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"petrol cars are cheaper than diesel cars\"\n",
    "doc2 = \"diesel is cheaper than petrol\"\n",
    "doc_corpus = [doc1,doc2]\n",
    "print(doc_corpus)\n",
    "print('')\n",
    "idf_vec = TfidfVectorizer(stop_words = 'english')\n",
    "idf_matrix = idf_vec.fit_transform(doc_corpus)\n",
    "idf_array = idf_matrix.toarray()\n",
    "print(\"Feature Names n\",idf_vec.get_feature_names())\n",
    "print('')\n",
    "print(\"Sparse Matrix n\",idf_matrix.shape,\"n\",idf_array)\n",
    "print('')\n",
    "df_idf = pd.DataFrame(data = idf_array,columns = idf_vec.get_feature_names())\n",
    "print(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c9268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petrol cars are cheaper than diesel cars', 'diesel is cheaper than petrol']\n",
      "\n",
      "Feature Names n ['are', 'cars', 'cheaper', 'diesel', 'is', 'petrol', 'than']\n",
      "\n",
      "Sparse Matrix n (2, 7) n [[0.37729199 0.75458397 0.26844636 0.26844636 0.         0.26844636\n",
      "  0.26844636]\n",
      " [0.         0.         0.4090901  0.4090901  0.57496187 0.4090901\n",
      "  0.4090901 ]]\n",
      "\n",
      "        are      cars   cheaper    diesel        is    petrol      than\n",
      "0  0.377292  0.754584  0.268446  0.268446  0.000000  0.268446  0.268446\n",
      "1  0.000000  0.000000  0.409090  0.409090  0.574962  0.409090  0.409090\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"petrol cars are cheaper than diesel cars\"\n",
    "doc2 = \"diesel is cheaper than petrol\"\n",
    "doc_corpus = [doc1,doc2]\n",
    "print(doc_corpus)\n",
    "print('')\n",
    "idf_vec = TfidfVectorizer()\n",
    "idf_matrix = idf_vec.fit_transform(doc_corpus)\n",
    "idf_array = idf_matrix.toarray()\n",
    "print(\"Feature Names n\",idf_vec.get_feature_names())\n",
    "print('')\n",
    "print(\"Sparse Matrix n\",idf_matrix.shape,\"n\",idf_array)\n",
    "print('')\n",
    "df_idf = pd.DataFrame(data = idf_array,columns = idf_vec.get_feature_names())\n",
    "print(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b72d9fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644f31d",
   "metadata": {},
   "source": [
    "#### Obs : Compare with the prev are, is ,than present when we are not passing are param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf1d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9, 'cycle': 1, 'is': 5, 'ridden': 7, 'on': 6, 'track': 10, 'bus': 0, 'driven': 2, 'road': 8, 'he': 4, 'driving': 3}\n",
      "[1.28768207 1.69314718 1.69314718 1.69314718 1.69314718 1.\n",
      " 1.28768207 1.69314718 1.69314718 1.         1.69314718]\n"
     ]
    }
   ],
   "source": [
    "# https://www.etutorialspoint.com/index.php/386-tf-idf-tfidfvectorizer-tutorial-with-examples\n",
    "text = [\"The cycle is ridden on the track.\",\n",
    "\t\"The bus is driven on the road.\",\n",
    "\t\"He is driving the bus.\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb805ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'any', 'document', 'first', 'here', 'is', 'letter', 'one', 'other', 'second', 'the', 'third', 'this']\n",
      "(4, 13)\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Here is the first letter.',\n",
    "    'This document is the second letter.',\n",
    "    'And this is the third one.',\n",
    "    'Is this any other letter?']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96b63e",
   "metadata": {},
   "source": [
    "### https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193d47b",
   "metadata": {},
   "source": [
    "# Train Document Set:\n",
    "- d1: The sky is blue.\n",
    "- d2: The sun is bright.\n",
    "# Test Document Set:\n",
    "- d3: The sun in the sky is bright.\n",
    "- d4: We can see the shining sun, the bright sun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a73bb6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "\n",
      "      blue  bright  sky  sun\n",
      "Doc1     1       0    1    0\n",
      "Doc2     0       1    0    1\n",
      "\n",
      "TD-IDF Vectorizer\n",
      "\n",
      "          blue    bright       sky       sun\n",
      "Doc1  0.707107  0.000000  0.707107  0.000000\n",
      "Doc2  0.000000  0.707107  0.000000  0.707107\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer \n",
    "# CountVectorizer\n",
    "# set of documents\n",
    "train = ['The sky is blue.','The sun is bright.']\n",
    "test = ['The sun in the sky is bright', 'We can see the shining sun, the bright sun.']\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "coun_vect = CountVectorizer(analyzer = 'word', stop_words='english')\n",
    "idf_vect = TfidfVectorizer(analyzer ='word',stop_words= 'english')\n",
    "\n",
    "# convert th documents into a matrix\n",
    "count_wm = coun_vect.fit_transform(train)\n",
    "tfidf_wm = idf_vect.fit_transform(train)\n",
    "\n",
    "#retrieve the terms found in the corpora\n",
    "# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , \n",
    "#it will give same output of get_feature_names() methods)\n",
    "#count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
    "count_tokens = coun_vect.get_feature_names()\n",
    "tfidf_tokens = idf_vect.get_feature_names()\n",
    "df_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4040b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n",
      "\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t2\n",
      "Sparse Matrix form of test data : \n",
      "\n",
      "[[0 1 1 1]\n",
      " [0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "#import count vectorize and tfidf vectorise\n",
    "train = ('The sky is blue.','The sun is bright.')\n",
    "test = ('The sun in the sky is bright', 'We can see the shining sun, the bright sun.')\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "# use analyzer is word and stop_words is english \n",
    "#which are responsible for remove stop words and create word vocabulary\n",
    "countvectorizer = CountVectorizer(analyzer = 'word' , stop_words = 'english')\n",
    "\n",
    "terms = countvectorizer.fit_transform(train)\n",
    "term_vectors  = countvectorizer.transform(test)\n",
    "print(terms)\n",
    "print('')\n",
    "print(term_vectors)\n",
    "print(\"Sparse Matrix form of test data : \\n\")\n",
    "print(term_vectors.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789bf0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector of idf \n",
      "\n",
      "[2.09861229 1.         1.40546511 1.        ]\n",
      "\n",
      "Final tf-idf vectorizer matrix form :\n",
      "\n",
      "[[0.         0.50154891 0.70490949 0.50154891]\n",
      " [0.         0.4472136  0.         0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Tranfer  sparse matrix of Countvectorizer to tf-idf by \n",
    "# using TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "term_vectors.todense()\n",
    "#[0, 1, 1, 1]\n",
    "# [0, 1, 0, 2]\n",
    "tfidf.fit(term_vectors)\n",
    "tf_idf_matrix = tfidf.transform(term_vectors)\n",
    "print(\"\\nVector of idf \\n\")\n",
    "print(tfidf.idf_)\n",
    "print(\"\\nFinal tf-idf vectorizer matrix form :\\n\")\n",
    "print(tf_idf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87a2d092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix form of test data : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.57735027, 0.57735027, 0.57735027],\n",
       "        [0.        , 0.4472136 , 0.        , 0.89442719]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the vectorizer object\n",
    "# use analyzer is word and stop_words is english which are responsible \n",
    "#for remove stop words and create word vocabulary\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word' , stop_words='english',)\n",
    "tfidfvectorizer.fit(train)\n",
    "tfidf_train = tfidfvectorizer.transform(train)\n",
    "tfidf_term_vectors  = tfidfvectorizer.transform(test)\n",
    "print(\"Sparse Matrix form of test data : \\n\")\n",
    "tfidf_term_vectors.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3c294",
   "metadata": {},
   "source": [
    "#### Obs : Here , we can see that both outputs are almost same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda94d82",
   "metadata": {},
   "source": [
    "## https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.Yn7yz5BX7Cg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c78133",
   "metadata": {},
   "source": [
    "### Tfidftransformer Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4de678a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 16)\n",
      "\n",
      "         idf_weights\n",
      "mouse       1.000000\n",
      "the         1.000000\n",
      "cat         1.693147\n",
      "house       1.693147\n",
      "ate         2.098612\n",
      "away        2.098612\n",
      "end         2.098612\n",
      "finally     2.098612\n",
      "from        2.098612\n",
      "had         2.098612\n",
      "little      2.098612\n",
      "of          2.098612\n",
      "ran         2.098612\n",
      "saw         2.098612\n",
      "story       2.098612\n",
      "tiny        2.098612\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Dataset and Imports\n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"]\n",
    "\n",
    "# 2. Initialize CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv = CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "print(word_count_vector.shape)\n",
    "print('')\n",
    "# 3. Compute the IDF values\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf = True,use_idf = True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names(),columns = [\"idf_weights\"]) \n",
    "# sort ascending \n",
    "print(df_idf.sort_values(by = ['idf_weights']))\n",
    "\n",
    "# 4. Compute the TFIDF score for your documents\n",
    "# count matrix \n",
    "count_vector = cv.transform(docs) ##  However, in practice, you may be computing tf-idf scores on a set of new unseen documents\n",
    "# tf-idf scores \n",
    "tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
    "\n",
    "feature_names = cv.get_feature_names() \n",
    "#get tfidf vector for first document \n",
    "first_document_vector = tf_idf_vector[0] \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index = feature_names, columns = [\"tfidf\"]) \n",
    "df.sort_values(by = [\"tfidf\"],ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44da0d",
   "metadata": {},
   "source": [
    "#### Obs : The scores above make sense. The more common the word across documents, the lower its score and the more unique a word is to our first document (e.g. ‘had’ and ‘tiny’) the higher the score. So it’s working as expected except for the mysterious a that was chopped off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dab28",
   "metadata": {},
   "source": [
    "## Tfidfvectorizer Usage \n",
    "- Now, we are going to use the same 5 documents from above to do the same thing as we did for Tfidftransformer – which is to get the tf-idf scores of a set of documents. But, notice how this is much shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f3e7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tfidf\n",
      "had      0.493562\n",
      "little   0.493562\n",
      "tiny     0.493562\n",
      "house    0.398203\n",
      "mouse    0.235185\n",
      "the      0.235185\n",
      "ate      0.000000\n",
      "away     0.000000\n",
      "cat      0.000000\n",
      "end      0.000000\n",
      "finally  0.000000\n",
      "from     0.000000\n",
      "of       0.000000\n",
      "ran      0.000000\n",
      "saw      0.000000\n",
      "story    0.000000\n"
     ]
    }
   ],
   "source": [
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf = True) \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index = tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "print(df.sort_values(by = [\"tfidf\"],ascending = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de082965",
   "metadata": {},
   "source": [
    "#### Obs : As expected same results comes with less coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81d903",
   "metadata": {},
   "source": [
    "## Tfidftransformer vs. Tfidfvectorizer\n",
    "\n",
    "- In summary, the main difference between the two modules are as follows:\n",
    "\n",
    "- With Tfidftransformer you will systematically compute word counts using CountVectorizer and then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores.\n",
    "\n",
    "- With Tfidfvectorizer on the contrary, you will do all three steps at once. Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048369f",
   "metadata": {},
   "source": [
    "## When to use what?\n",
    "- So now you may be wondering, why you should use more steps than necessary if you can get everything done in two steps. Well, there are cases where you want to use Tfidftransformer over Tfidfvectorizer and it is sometimes not that obvious. Here is a general guideline:\n",
    "\n",
    "    - If you need the term frequency (term count) vectors for different tasks, use Tfidftransformer.\n",
    "    - If you need to compute tf-idf scores on documents within your “training” dataset, use Tfidfvectorizer\n",
    "    - If you need to compute tf-idf scores on documents outside your “training” dataset, use either one, both will work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda2dcd",
   "metadata": {},
   "source": [
    "### https://www.analyticsvidhya.com/blog/2021/07/bag-of-words-vs-tfidf-vectorization-a-hands-on-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e756e8",
   "metadata": {},
   "source": [
    "## Bag-of-words vs TFIDF vectorization –A Hands-on Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4cc93",
   "metadata": {},
   "source": [
    "### Bag-of-words using Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3c8c946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'easy', 'important', 'is', 'necessary', 'processing', 'text']\n",
      "\n",
      "[[0 0 0 1 1 1 1]\n",
      " [1 0 1 1 1 1 1]\n",
      " [0 1 0 1 0 1 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = ['Text processing is necessary.',\n",
    "          'Text processing is necessary and important.',\n",
    "          'Text processing is easy.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print('')\n",
    "print(X.toarray())\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29720a2",
   "metadata": {},
   "source": [
    "### TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d792e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.46333427 0.59662724 0.46333427\n",
      "  0.46333427]\n",
      " [0.52523431 0.         0.52523431 0.31021184 0.39945423 0.31021184\n",
      "  0.31021184]\n",
      " [0.         0.69903033 0.         0.41285857 0.         0.41285857\n",
      "  0.41285857]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe6d50",
   "metadata": {},
   "source": [
    "## Important parameters to know – Sklearn’s CountVectorizer & TFIDF vectorization:\n",
    "\n",
    "   - max_features: This parameter enables using only the ‘n’ most frequent words as features instead of all the words. An integer can be passed for this parameter.\n",
    "   - stop_words: You could remove the extremely common words like ‘this’, ’is’, ’are’ etc by using this parameter as the common words add little value to the model. We can set the parameter to ‘english’ to use a built-in list. We can also set this parameter to a custom list.\n",
    "   - analyzer: This parameter tells the model if the feature should be made of word n-grams or character n-grams. We can set it to be ‘word’, ‘char’ or ‘char_wb’. Option ‘char_wb’ creates character n-grams only from text inside word boundaries.\n",
    "   - ngram_range: An n-gram is a string of words in a row. For example, in the sentence – “Text processing is easy.”, 2-grams could be ‘Text processing’, ‘processing is’ or ‘is easy’. We can set the ngram_range to be (x,y) where x is the minimum and y is the maximum size of the n-grams we want to include in the features. The default ngram_range is (1,1).\n",
    "   - min_df, max_df: These refer to the minimum and maximum document frequency that a word/n-gram should have to be used as a feature. The frequency here refers to the proportion of documents. Both the parameters have to be set in the range of [0,1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d4a10",
   "metadata": {},
   "source": [
    "## Machine Learning Techniques for Text Representation in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3434a9",
   "metadata": {},
   "source": [
    "### https://www.analyticsvidhya.com/blog/2022/02/machine-learning-techniques-for-text-representation-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdaa77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c69cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b543333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b1d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00158e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d466237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd1ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc55a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce324b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f28cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4a6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
